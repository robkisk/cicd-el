# Custom section is used to store configurations that might be repetative.
# Please read YAML documentation for details on how to use substitutions and anchors.

custom:
  default-cluster-spec: &default-cluster-spec
    spark_version: "{{ var['TASK_CLUSTER']['SPARK_VERSION'] }}"
    spark_conf:
      spark.databricks.delta.preview.enabled: "true"
    node_type_id: "{{ var['TASK_CLUSTER']['NODE_TYPE'] }}"
    driver_node_type_id: "{{ var['TASK_CLUSTER']['DRIVER_NODE_TYPE'] }}"
    num_workers: 1

  dev-cluster-config: &dev-cluster-config
    new_cluster:
      <<: *default-cluster-spec

  staging-cluster-config: &staging-cluster-config
    new_cluster:
      <<: *default-cluster-spec

  prod-cluster-config: &prod-cluster-config
    new_cluster:
      <<: *default-cluster-spec

  notification: &notification
    email_notifications:
      on_start: ["{{ var['EMAIL'] }}"]
      on_success: ["{{ var['EMAIL'] }}"]
      on_failure: ["{{ var['EMAIL'] }}"]
environments:
  default:
    workflows:
      #######################################################################################
      #   Example workflow for integration tests                                            #
      #######################################################################################
      - name: "cicd-el-sample-tests"
        tasks:
          - task_key: "main"
            <<: *dev-cluster-config
            spark_python_task:
              python_file: "file://tests/entrypoint.py"
              # this call supports all standard pytest arguments
              parameters: ["file:fuse://tests/integration", "--cov=cicd_el"]
      #######################################################################################
      # this is an example job with single ETL task based on 2.1 API and wheel_task format #
      ######################################################################################
      - name: "cicd-el-sample-etl"
        tasks:
          - task_key: "main"
            <<: *dev-cluster-config
            python_wheel_task:
              package_name: "cicd_el"
              entry_point: "etl" # take a look at the setup.py entry_points section for details on how to define an entrypoint
              parameters:
                ["--conf-file", "file:fuse://conf/tasks/sample_etl_config.yml"]
      #############################################################
      # this is an example multitask job with notebook task       #
      #############################################################
      - name: "cicd-el-sample-multitask"
        git_source:
          git_url: https://github.com/robkisk/el-cicd
          git_provider: "github"
          git_branch: "main"
        tags:
          your-key: "Owner"
          your-key1: "robby.kiskanyan@databricks.com"
        job_clusters:
          - job_cluster_key: "default"
            <<: *dev-cluster-config
        tasks:
          - task_key: "etl"
            job_cluster_key: "default"
            spark_python_task:
              python_file: "file://cicd_el/tasks/sample_etl_task.py"
              parameters:
                ["--conf-file", "file:fuse://conf/tasks/sample_etl_config.yml"]
          - task_key: "ml"
            depends_on:
              - task_key: "etl"
            job_cluster_key: "default"
            python_wheel_task:
              package_name: "cicd_el"
              entry_point: "ml"
              parameters:
                ["--conf-file", "file:fuse://conf/tasks/sample_ml_config.yml"]
          ###############################################################################
          # this is an example task based on the notebook                               #
          # Please note that first you'll need to add a Repo and commit notebook to it. #
          ###############################################################################
          - task_key: "notebook"
            deployment_config:
              no_package: true # we omit using package since code will be shipped directly from the Repo
            depends_on:
              - task_key: "ml"
            job_cluster_key: "default"
            notebook_task:
              # notebook_path: "/Repos/Staging/cicd_el/notebooks/sample_notebook"
              notebook_path: "notebooks/sample_notebook"
